Absolutely, Abhinav. Here‚Äôs your fully revamped and comprehensive Devin prompt, incorporating database-backed configuration, Java-based enrichment modules, modular architecture, percentage-based row count validation, and a robust orchestrator design.

This prompt is ready to use as-is for Devin or any engineering team to design, architect, and build the Master Archive Engine end-to-end.

‚∏ª

‚úÖ Devin Prompt: Build a Config-Driven, Modular Master Archive Engine

‚∏ª

üéØ Objective:

Design and implement a generic, scalable, and pluggable Master Archive Engine that enables flexible, file-based data ingestion pipelines driven by configurations stored in a relational database. Every file must pass through a series of modular processing steps: archival, validation, enrichment, and loading.

The system must be fully configurable via the database, highly extensible, and allow different file types to follow different pipelines. The enrichment logic must be decoupled from the core engine and implemented as Java helper classes that can be executed dynamically as part of the pipeline.

‚∏ª

üß± Core Architectural Principles:
	‚Ä¢	Modularity: Each pipeline step (archive, validate, enrich, load) is a standalone module with clear input/output contracts.
	‚Ä¢	Configuration-driven: All behaviors are controlled via DB configuration ‚Äî no static YAML/JSON files.
	‚Ä¢	Extensibility: Support for new file types, validation rules, enrichment logic, and loading targets with minimal effort.
	‚Ä¢	Auditability: Complete logging of file processing lifecycle and validation outcomes.
	‚Ä¢	Technology-neutral Enrichment: Enrichment is done via external Java classes invoked dynamically.

‚∏ª

üì¶ Core Modules and Responsibilities:

‚∏ª

1. üì• Arrival Watcher Service
	‚Ä¢	Monitors a predefined Arrival Folder (local path or S3 bucket).
	‚Ä¢	Detects new files and matches them against configured file type patterns.
	‚Ä¢	Triggers the orchestrator with file metadata.
	‚Ä¢	Supports:
	‚Ä¢	Pattern-based file type detection
	‚Ä¢	Cron-based polling or filesystem event triggers

‚∏ª

2. üì¶ Archive Service
	‚Ä¢	Immediately archives every incoming file before any further processing.
	‚Ä¢	Archive destination is configured per file type (e.g., S3 path or backup folder).
	‚Ä¢	Supports file versioning using:
	‚Ä¢	Timestamps
	‚Ä¢	UUIDs
	‚Ä¢	Hashes (for de-duplication)
	‚Ä¢	Fails the pipeline if archiving is unsuccessful.
	‚Ä¢	Optionally logs archive metadata (file size, timestamp, archive path).

‚∏ª

3. ‚úÖ Validation Service
	‚Ä¢	Performs data validation in 3 layers:
	‚Ä¢	Schema Validation: Required columns, header checks, data types.
	‚Ä¢	Business Rule Validation: Regex, nullability, ID formats, etc.
	‚Ä¢	Row Count Threshold Validation (Percentage-Based):
	‚Ä¢	Uses a configurable baseline row count.
	‚Ä¢	Ensures the file has rows within a defined percentage range (e.g., 80%‚Äì120%).
	‚Ä¢	Prevents processing of abnormally large/small files.
	‚Ä¢	Validation results include:
	‚Ä¢	Detailed error logs (field name, row number, issue)
	‚Ä¢	Summary report (validation status, failure reason)

‚∏ª

4. ‚ú® Enrichment Service
	‚Ä¢	Executes external Java helper classes to enrich file data.
	‚Ä¢	Each file type has an associated Java class that implements a standard interface (e.g., EnrichmentProcessor).
	‚Ä¢	The main engine invokes these classes via:
	‚Ä¢	Shell execution (if outside JVM)
	‚Ä¢	Reflection (if running within Java)
	‚Ä¢	Enrichment logic may include:
	‚Ä¢	Adding derived/calculated fields
	‚Ä¢	Mapping codes to descriptions
	‚Ä¢	External API lookups or DB joins
	‚Ä¢	The Java class must accept an input file path and return an enriched output file path (or overwrite in-place).

‚∏ª

5. üöÄ Loader Service
	‚Ä¢	Loads the final output into a target database.
	‚Ä¢	Target table, column mappings, and load strategy (insert/upsert) are all stored in DB.
	‚Ä¢	Supports:
	‚Ä¢	Batch inserts
	‚Ä¢	Transaction rollback on failure
	‚Ä¢	Logging the number of records inserted, failed, or skipped
	‚Ä¢	Validates schema compatibility before loading.

‚∏ª

6. üß† Pipeline Orchestrator
	‚Ä¢	Coordinates the execution of all steps per file based on configuration.
	‚Ä¢	Reads the sequence of steps and their configurations from the database:
	‚Ä¢	archive ‚Üí validate ‚Üí enrich ‚Üí load
	‚Ä¢	Logs the execution status of each step.
	‚Ä¢	Handles:
	‚Ä¢	Conditional execution (e.g., skip enrichment if not configured)
	‚Ä¢	Retry logic for failed steps
	‚Ä¢	Exception handling and recovery
	‚Ä¢	Emits processing lifecycle events/logs.

‚∏ª

üóÉÔ∏è Database Schema (Finalized Config Tables)

‚∏ª

1. file_types

Defines supported file types.

Column	Description
id (PK)	Unique ID
name	e.g., ‚Äúemployee_data‚Äù
file_pattern	Regex/pattern to identify file type
expected_extension	csv, xlsx, json, etc.
enabled	Whether processing is active for this type


‚∏ª

2. pipeline_steps

Defines ordered processing steps for each file type.

Column	Description
file_type_id (FK)	From file_types
step_order	Execution order (1=archive, 2=validate, etc.)
step_type	One of: archive, validate, enrich, load
enabled	Whether this step should run
config_ref_id	Points to related config table row


‚∏ª

3. validation_rules

Validation logic for each file type.

Column	Description
file_type_id (FK)	
required_columns	JSON list of required columns
regex_validations	JSON object with column ‚Üí regex mappings
row_count_baseline	Integer expected baseline rows
min_percentage	Minimum % of baseline (e.g., 80)
max_percentage	Maximum % of baseline (e.g., 120)
strict_mode	Fail immediately if any rule fails


‚∏ª

4. enrichment_rules

Enrichment logic per file type.

Column	Description
file_type_id (FK)	
class_name	Fully qualified Java class (e.g., com.enrich.EmployeeRoleMapper)
parameters	Optional JSON object with config parameters
enabled	


‚∏ª

5. load_configs

Defines destination table and strategy.

Column	Description
file_type_id (FK)	
target_table	e.g., ‚Äúemployee_master‚Äù
column_mappings	JSON of file ‚Üí DB column map
load_mode	insert, upsert, truncate_insert
batch_size	Number of rows per insert batch


‚∏ª

6. processing_logs and step_logs

Tracks status of every file and each step during processing.

‚∏ª

üß™ Sample Processing Scenarios

‚∏ª

‚úÖ Example 1: Employee File (employee.csv)
	‚Ä¢	Pattern: employee_*.csv
	‚Ä¢	Pipeline: archive ‚Üí validate ‚Üí enrich ‚Üí load
	‚Ä¢	Validation:
	‚Ä¢	Required columns: id, name, role_code
	‚Ä¢	Row count baseline: 10,000 (acceptable range: 8,000‚Äì12,000)
	‚Ä¢	Enrichment: Map role_code ‚Üí role_name via Java class
	‚Ä¢	Load: Insert into employees table

‚∏ª

‚úÖ Example 2: Product Master (product.xlsx)
	‚Ä¢	Pipeline: archive ‚Üí validate ‚Üí load
	‚Ä¢	No enrichment
	‚Ä¢	Regex validation for sku_code

‚∏ª

‚úÖ Example 3: JSON Feed (orders.json)
	‚Ä¢	Pipeline: archive ‚Üí load
	‚Ä¢	No validation or enrichment

‚∏ª

üì° Monitoring and Observability
	‚Ä¢	Log every file lifecycle event with:
	‚Ä¢	File name, type, processing time
	‚Ä¢	Step status: Success / Fail / Skipped
	‚Ä¢	Failure reason if applicable
	‚Ä¢	Expose health check endpoints per service
	‚Ä¢	Optionally export metrics to Prometheus or dashboard (Grafana)

‚∏ª

üîß Tech Stack Guidelines (Flexible)

Layer	Suggested Tech Options
Core engine	Python / Node.js
Config storage	PostgreSQL / MySQL
Enrichment logic	Java helper classes
File I/O	Local FS + S3 (via SDK)
Orchestration	Cron jobs / Internal queue / Event triggers
Logging	JSON logs or centralized log store
Monitoring	REST health endpoints / Grafana (optional)


‚∏ª

üìÅ Deliverables
	‚Ä¢	Well-structured modular codebase
	‚Ä¢	SQL schema for all config and tracking tables
	‚Ä¢	Example entries for 2‚Äì3 file types
	‚Ä¢	Sample Java enrichment class interface and demo implementation
	‚Ä¢	README and setup instructions
	‚Ä¢	Local test scripts and logs

‚∏ª

Let me know if you‚Äôd also like:
	‚Ä¢	‚úÖ A high-level architecture diagram
	‚Ä¢	‚úÖ Sample SQL inserts for seeding config
	‚Ä¢	‚úÖ Java enrichment interface template
	‚Ä¢	‚úÖ CI/CD or deployment setup guidance

This is now a fully enterprise-ready foundation for a flexible data ingestion and pipeline automation system.